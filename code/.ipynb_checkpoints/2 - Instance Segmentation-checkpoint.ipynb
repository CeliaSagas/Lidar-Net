{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca336aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import os.path as osp\n",
    "from nuscenes import NuScenes\n",
    "\n",
    "# Utils for Lidar and Radar\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from utils.misc import *\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2df54c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.489 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "#establishing root data and folder for nuscenes\n",
    "nuscenes_root = \"/users/CeliaSagastume/Desktop/Lidar-Net/data/sets/nuscenes\"\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=nuscenes_root, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d37b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Segment all point clouds in the dataset and save them with corresponding object label\n",
    "\n",
    "def Save_PointClouds(FilePath):\n",
    "    \n",
    "    #define samples path\n",
    "    samples_path = FilePath\n",
    "\n",
    "    # define path to save point clouds in .csv\n",
    "    csv_path =  os.getcwd() + '/dataset/data/'\n",
    "\n",
    "    #Dictionary to store point clouds \n",
    "    pc_dict = defaultdict( lambda: defaultdict(list))\n",
    "\n",
    "    #Dictionary to store categories\n",
    "    cat_dict = defaultdict(list)\n",
    "\n",
    "    # extract all files in path\n",
    "    for root, dirs, files in os.walk(samples_path):\n",
    "\n",
    "        # select each sample file\n",
    "        for file_ in tqdm(files, desc = \"Sample files\"):   \n",
    "\n",
    "            # load the current sample file\n",
    "            data_json = load_file(samples_path + file_)\n",
    "\n",
    "            # process each instance\n",
    "            for index in range(len(data_json['instance'])):\n",
    "\n",
    "                # Create instance metadata\n",
    "                annotation_metadata = data_json['instance'][index]\n",
    "\n",
    "                #Retrieve instance token \n",
    "                category_name = data_json['instance'][index]['category']\n",
    "\n",
    "                #Retrieve annotation token\n",
    "                annotation = annotation_metadata['annotation_token']\n",
    "\n",
    "                # point cloud\n",
    "                pointcloud_lidar = load_pcl_txt(annotation_metadata['pointcloud_path'], annotation_metadata['pcl_shape'])\n",
    "\n",
    "\n",
    "                clustering = DBSCAN(eps = 1.75, min_samples = 20).fit(pointcloud_lidar.T)\n",
    "                # Clusters\n",
    "                dbscan_clusters = clustering.labels_\n",
    "\n",
    "                # Obtain the more repeated index different of -1\n",
    "                class_idx = np.bincount(dbscan_clusters[dbscan_clusters != -1]).argmax()\n",
    "                # Find indexes of interest cluster\n",
    "                indexes = np.where(dbscan_clusters == class_idx)[0]\n",
    "\n",
    "                # extract the bigger cluster\n",
    "                pcl_temp = pointcloud_lidar.T[indexes]\n",
    "                \n",
    "                #create point cloud array\n",
    "                Velopoints = np.asarray(pcl_temp, np.float32)\n",
    "\n",
    "                   #add point cloud to dictionary \n",
    "                pc_dict[annotation]['pointcloud'].append(Velopoints.tolist())\n",
    "\n",
    "                #add category label to category dictionary\n",
    "                pc_dict[annotation]['category'].append(category_name)\n",
    "\n",
    "            #convert dictionary to json file and save for modelling \n",
    "            with open('pc_data.txt', 'w') as fp:\n",
    "                json.dump(pc_dict, fp,  indent=4)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a7998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample files: 100%|███████████████████████████| 404/404 [10:33<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "Path = os.getcwd() + '/dataset/samples/'\n",
    "Save_PointClouds(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8e0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
